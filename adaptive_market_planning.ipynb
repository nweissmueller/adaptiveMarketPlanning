{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class State:\n",
    "    order_quantity: float\n",
    "    counter: int\n",
    "\n",
    "@dataclass\n",
    "class Decision:\n",
    "    step_size: float\n",
    "\n",
    "class AdaptiveMarketPlanningModel:\n",
    "    \"\"\"Base class for the Adaptive Market Planning model.\"\"\"\n",
    "\n",
    "    def __init__(self, s_0, T, reward_type, price=1.0, cost=1.0, seed=20180613):\n",
    "        \"\"\"\n",
    "        Initialize the Adaptive Market Planning model.\n",
    "\n",
    "        :param s_0: Initial state dictionary\n",
    "        :param T: Number of time steps\n",
    "        :param reward_type: Type of reward ('Cumulative' or 'Terminal')\n",
    "        :param price: Price of the product (default: 1.0)\n",
    "        :param cost: Cost of the product (default: 1.0)\n",
    "        :param seed: Random seed for reproducibility (default: 20180613)\n",
    "        \"\"\"\n",
    "        self.prng = np.random.RandomState(seed)\n",
    "        self.state = State(*s_0.values())\n",
    "        self.T = T\n",
    "        self.reward_type = reward_type\n",
    "        self.obj = 0.0\n",
    "        self.past_derivative = 0.0\n",
    "        self.cost = cost\n",
    "        self.price = price\n",
    "        self.t = 0\n",
    "        self.learning_list = []\n",
    "\n",
    "    def exog_info_fn(self, decision):\n",
    "        \"\"\"\n",
    "        Generate exogenous information (demand) based on the decision.\n",
    "\n",
    "        :param decision: Decision object\n",
    "        :return: Dictionary containing the demand information\n",
    "        \"\"\"\n",
    "        return {'demand': self.prng.exponential(100)}\n",
    "\n",
    "    def transition_fn(self, decision, exog_info):\n",
    "        \"\"\"\n",
    "        Update the state based on the decision and exogenous information.\n",
    "\n",
    "        :param decision: Decision object\n",
    "        :param exog_info: Dictionary containing the exogenous information\n",
    "        \"\"\"\n",
    "        self.learning_list.append(self.state.order_quantity)\n",
    "        derivative = self.price - self.cost if self.state.order_quantity < exog_info['demand'] else -self.cost\n",
    "        new_order_quantity = max(0, self.state.order_quantity + decision.step_size * derivative)\n",
    "        new_counter = self.state.counter + (self.past_derivative * derivative < 0)\n",
    "        self.past_derivative = derivative\n",
    "        self.state = State(new_order_quantity, new_counter)\n",
    "\n",
    "    def objective_fn(self, decision, exog_info):\n",
    "        \"\"\"\n",
    "        Calculate the objective function value based on the decision and exogenous information.\n",
    "\n",
    "        :param decision: Decision object\n",
    "        :param exog_info: Dictionary containing the exogenous information\n",
    "        :return: Objective function value\n",
    "        \"\"\"\n",
    "        return self.price * min(self.state.order_quantity, exog_info['demand']) - self.cost * self.state.order_quantity\n",
    "\n",
    "    def step(self, decision):\n",
    "        \"\"\"\n",
    "        Perform a single step of the Adaptive Market Planning model.\n",
    "\n",
    "        :param decision: Decision object\n",
    "        \"\"\"\n",
    "        self.t += 1\n",
    "        exog_info = self.exog_info_fn(decision)\n",
    "        onestep_contribution = self.objective_fn(decision, exog_info)\n",
    "        self.obj += onestep_contribution if self.reward_type == 'Cumulative' else (onestep_contribution if self.t == self.T else 0)\n",
    "        self.transition_fn(decision, exog_info)\n",
    "\n",
    "class AdaptiveMarketPlanningPolicy:\n",
    "    \"\"\"Base class for the Adaptive Market Planning policy.\"\"\"\n",
    "\n",
    "    def __init__(self, model, theta_step):\n",
    "        \"\"\"\n",
    "        Initialize the Adaptive Market Planning policy.\n",
    "\n",
    "        :param model: Adaptive Market Planning model object\n",
    "        :param theta_step: Step size parameter for the policy\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.theta_step = theta_step\n",
    "\n",
    "    def kesten_rule(self):\n",
    "        \"\"\"\n",
    "        Apply Kesten's rule to determine the step size.\n",
    "\n",
    "        :return: Decision object with the calculated step size\n",
    "        \"\"\"\n",
    "        return Decision(self.theta_step / (self.theta_step + self.model.state.counter - 1))\n",
    "\n",
    "    def run_policy(self):\n",
    "        \"\"\"\n",
    "        Run the Adaptive Market Planning policy for the specified number of time steps.\n",
    "\n",
    "        :return: Tuple containing the final objective value and the learning list\n",
    "        \"\"\"\n",
    "        for _ in range(self.model.T):\n",
    "            self.model.step(self.kesten_rule())\n",
    "        return self.model.obj, self.model.learning_list.copy()\n",
    "\n",
    "# Set up the model parameters\n",
    "cost, price, theta_step, T, reward_type = 20, 26, 50, 24, \"Terminal\"\n",
    "trial_size = 100\n",
    "\n",
    "# Initialize the model and policy\n",
    "init_state = {'order_quantity': 0, 'counter': 0}\n",
    "model = AdaptiveMarketPlanningModel(init_state, T, reward_type, price, cost)\n",
    "policy = AdaptiveMarketPlanningPolicy(model, theta_step)\n",
    "\n",
    "# Run multiple iterations of the policy\n",
    "rewards_per_iteration = np.zeros(trial_size)\n",
    "learning_list_per_iteration = []\n",
    "for ite in range(trial_size):\n",
    "    model.t = 0\n",
    "    model.obj = 0.0\n",
    "    model.learning_list = []\n",
    "    reward, learning_list = policy.run_policy()\n",
    "    rewards_per_iteration[ite] = reward\n",
    "    learning_list_per_iteration.append(learning_list)\n",
    "\n",
    "# Calculate cumulative average rewards\n",
    "nElem = np.arange(1, trial_size + 1)\n",
    "rewards_per_iteration_cum_avg = rewards_per_iteration.cumsum() / nElem\n",
    "if reward_type == \"Cumulative\":\n",
    "    rewards_per_iteration_cum_avg /= T\n",
    "    rewards_per_iteration /= T\n",
    "\n",
    "# Calculate the optimal order quantity\n",
    "optimal_order_quantity = -np.log(cost / price) * 100\n",
    "\n",
    "# Print the results\n",
    "print(f\"Optimal order_quantity for price {price} and cost {cost} is {optimal_order_quantity:.2f}\")\n",
    "print(f\"Reward type: {reward_type}, theta_step: {theta_step}, T: {T} - Average reward over {trial_size} iterations is: {rewards_per_iteration_cum_avg[-1]:.2f}\")\n",
    "\n",
    "# Select a random iteration for plotting\n",
    "ite = np.random.randint(trial_size)\n",
    "order_quantity = learning_list_per_iteration[ite]\n",
    "print(f\"Order quantity for iteration {ite}\")\n",
    "print(order_quantity)\n",
    "\n",
    "# Plot the reward trends\n",
    "fig1, axsubs = plt.subplots(1, 2, figsize=(14, 5), sharex=True, sharey=True)\n",
    "fig1.suptitle(f\"Reward type: {reward_type}, theta_step: {theta_step}, T: {T}\")\n",
    "\n",
    "axsubs[0].plot(nElem, rewards_per_iteration_cum_avg, 'g')\n",
    "axsubs[0].set_title('Cumulative Average Reward')\n",
    "\n",
    "axsubs[1].plot(nElem, rewards_per_iteration, 'b')\n",
    "axsubs[1].set_title('Reward Per Iteration')\n",
    "\n",
    "ax = fig1.add_subplot(111, frameon=False)\n",
    "plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
    "ax.set_ylabel('USD', labelpad=15)\n",
    "ax.set_xlabel('Iterations', labelpad=15)\n",
    "plt.show()\n",
    "\n",
    "# Plot the analytical vs. learned order quantity\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Order Quantity\")\n",
    "plt.title(f\"Analytical vs. Learned Order Quantity - Iteration {ite}\")\n",
    "time = np.arange(len(order_quantity))\n",
    "plt.plot(time, [optimal_order_quantity] * len(time), 'r--', label=\"Analytical Solution\")\n",
    "plt.plot(time, order_quantity, 'b', label=f\"Learned (Kesten's Rule, Î¸={theta_step})\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
